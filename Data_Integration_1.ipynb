{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad94fda8",
   "metadata": {},
   "source": [
    "### MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe06dc7-7519-4106-b168-34e7d6572cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/azureuser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/azureuser/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b7d81c65-d525-4dc7-9f52-12db970b4551;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 219ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b7d81c65-d525-4dc7-9f52-12db970b4551\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/9ms)\n",
      "24/02/21 06:09:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+\n",
      "|CITY     |NUM_OF_CUSTOMERS|TOTAL_SALES|\n",
      "+---------+----------------+-----------+\n",
      "|San Diego|346             |7540       |\n",
      "|New York |501             |11258      |\n",
      "|Austin   |676             |15351      |\n",
      "|London   |794             |17710      |\n",
      "|Seattle  |1134            |24853      |\n",
      "|Denver   |1549            |34155      |\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,explode\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDBMflixAnalysis\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", f\"mongodb+srv://student:student@cluster0.koi0v.mongodb.net\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", f\"mongodb+srv://student:student@cluster0.koi0v.mongodb.net\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "sales_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb+srv://student:student@cluster0.koi0v.mongodb.net/sample_supplies.sales\").load()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Select columns and explode the \"items\" array\n",
    "customer_and_sales = sales_df.select(\n",
    "    sales_df[\"storeLocation\"].alias(\"CITY\"), \n",
    "    sales_df[\"customer\"],\n",
    "    explode(\"items\").alias(\"item\")\n",
    ")\n",
    "\n",
    "# Calculate total sales by city\n",
    "total_sales_by_city = customer_and_sales.groupBy(\"CITY\").agg(\n",
    "    F.sum(F.col(\"item.quantity\")).alias(\"TOTAL_SALES\")\n",
    ")\n",
    "\n",
    "# Calculate total customers by city\n",
    "total_customers_by_city = customer_and_sales.groupBy(\"CITY\").agg(\n",
    "    F.countDistinct(\"customer\").alias(\"NUM_OF_CUSTOMERS\")\n",
    ")\n",
    "\n",
    "# Join the two DFs on the \"City\" column\n",
    "df_mongodb = total_customers_by_city.join(\n",
    "    total_sales_by_city, \n",
    "    \"CITY\", \n",
    "    \"inner\"\n",
    ").orderBy(\"TOTAL_SALES\")\n",
    "\n",
    "df_mongodb.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2319b65b",
   "metadata": {},
   "source": [
    "### Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e8866d-1bd3-49f9-b241-d2c705295a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 06:09:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------------------+\n",
      "|          CITY|NUM_OF_CUSTOMERS|         TOTAL_SALES|\n",
      "+--------------+----------------+--------------------+\n",
      "|     Australia|              33|   1421810.923837996|\n",
      "|       Germany|              32|  1827066.7117509968|\n",
      "|          NULL|              87|  1997407.4910679972|\n",
      "|        France|              34|   4509888.930791994|\n",
      "|     Southeast|              74|   7171012.749148975|\n",
      "|United Kingdom|              62|   8503338.645407949|\n",
      "|     Northeast|             118|   9293903.004376972|\n",
      "|     Northwest|              71|   9367593.632682951|\n",
      "|        Canada|             105|   9535865.570011947|\n",
      "|       Central|             121|1.0065803540256996E7|\n",
      "|     Southwest|             104| 1.679401297999503E7|\n",
      "+--------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"AdventureWorksAnalysis\").getOrCreate()\n",
    "\n",
    "# Set the Hadoop file path\n",
    "hadoop_path = \"hdfs://localhost:9000/user/input/adventureworks/\"\n",
    "\n",
    "# Read CSV files\n",
    "customers = spark.read.csv(hadoop_path + \"customers.csv\", header=True, inferSchema=True)\n",
    "employees = spark.read.csv(hadoop_path + \"employees.csv\", header=True, inferSchema=True)\n",
    "orders = spark.read.csv(hadoop_path + \"orders.csv\", header=True, inferSchema=True)\n",
    "product_categories = spark.read.csv(hadoop_path + \"productcategories.csv\", header=True, inferSchema=True)\n",
    "products = spark.read.csv(hadoop_path + \"products.csv\", header=True, inferSchema=True)\n",
    "product_subcategories = spark.read.csv(hadoop_path + \"productsubcategories.csv\", header=True, inferSchema=True)\n",
    "vendor_product = spark.read.csv(hadoop_path + \"vendorproduct.csv\", header=True, inferSchema=True)\n",
    "vendors = spark.read.csv(hadoop_path + \"vendors.csv\", header=True, inferSchema=True)\n",
    "\n",
    "joined_df = orders.join(employees, orders[\"EmployeeID\"] == employees[\"EmployeeID\"], \"inner\")\n",
    "\n",
    "# Calculate total sales by city\n",
    "total_sales = joined_df.groupBy(\"Territory\") \\\n",
    "                       .agg(sum(\"LineTotal\").alias(\"TOTAL_SALES\")) \\\n",
    "                       .withColumnRenamed(\"Territory\", \"CITY\")\n",
    "\n",
    "# Calculate total customers by city\n",
    "distinct_customers = joined_df.groupBy(\"Territory\") \\\n",
    "                              .agg(countDistinct(\"CustomerID\").alias(\"NUM_OF_CUSTOMERS\")) \\\n",
    "                              .withColumnRenamed(\"Territory\", \"CITY\")\n",
    "\n",
    "#total_revenue = joined_df.groupBy(\"Territory\") \\\n",
    "#                         .agg((sum(\"LineTotal\") - sum(\"UnitPrice\")).alias(\"Total_Revenue\")) \\\n",
    "#                         .withColumnRenamed(\"Territory\", \"city\")\n",
    "\n",
    "# Join the DFs together\n",
    "df_hadoop = distinct_customers.join(total_sales, \"CITY\") \\\n",
    "                    .orderBy(\"TOTAL_SALES\")\n",
    "                    #.join(total_revenue, \"city\") \\\n",
    "                    \n",
    "df_hadoop.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d641db85",
   "metadata": {},
   "source": [
    "### MARIADB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6cb53d-9c01-4ad0-818f-ae96c449a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 06:10:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+\n",
      "|         city|NUM_OF_CUSTOMERS|TOTAL_SALES|\n",
      "+-------------+----------------+-----------+\n",
      "|       Madrid|               5|11099786.57|\n",
      "|   San Rafael|               1| 5326446.06|\n",
      "|          NYC|               5| 1642508.02|\n",
      "|   Auckland  |               2| 1030870.69|\n",
      "|    Singapore|               3|  948244.37|\n",
      "|    Melbourne|               1|  722340.28|\n",
      "|        Paris|               3|  721949.04|\n",
      "|  New Bedford|               2|  571500.03|\n",
      "| North Sydney|               1|  548136.88|\n",
      "|       Nantes|               2|  542662.44|\n",
      "|    Kobenhavn|               1|  516340.48|\n",
      "|        Reims|               1|  507932.76|\n",
      "|San Francisco|               2|  464813.24|\n",
      "|   Manchester|               1|  445230.27|\n",
      "|    Minato-ku|               1|  422194.92|\n",
      "|   Burlingame|               1|  418180.88|\n",
      "|      Stavern|               1|  416899.16|\n",
      "|     Salzburg|               1|  412440.21|\n",
      "|    Chatswood|               1|  401721.36|\n",
      "|   Brickhaven|               3|  396418.25|\n",
      "+-------------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"PySpark Example\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Select the database on MariaDB\n",
    "server = \"localhost\"\n",
    "port = 3306\n",
    "database = \"classicmodels\"\n",
    "jdbc_url = f\"jdbc:mysql://{server}:{port}/{database}?permitMysqlScheme\"\n",
    "\n",
    "# Login\n",
    "user = \"haunguyen2\"\n",
    "password = \"haunguyenpwd\"\n",
    "jdbc_driver = \"org.mariadb.jdbc.Driver\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": jdbc_driver\n",
    "}\n",
    "\n",
    "# Read .sql files then create their own DFs\n",
    "customers_df = spark.read.jdbc(jdbc_url, \"(select * from customers) tab\", properties=properties)\n",
    "employees_df = spark.read.jdbc(jdbc_url, \"(select * from employees) tab\", properties=properties)\n",
    "offices_df = spark.read.jdbc(jdbc_url, \"(select * from offices) tab\", properties=properties)\n",
    "orderdetails_df = spark.read.jdbc(jdbc_url, \"(select * from orderdetails) tab\", properties=properties)\n",
    "orders_df = spark.read.jdbc(jdbc_url, \"(select * from orders) tab\", properties=properties)\n",
    "payments_df = spark.read.jdbc(jdbc_url, \"(select * from payments) tab\", properties=properties)\n",
    "productlines_df = spark.read.jdbc(jdbc_url, \"(select * from productlines) tab\", properties=properties)\n",
    "products_df = spark.read.jdbc(jdbc_url, \"(select * from products) tab\", properties=properties)\n",
    "\n",
    "# Join relevant data tgt with total sales and total customers by city\n",
    "df_maria = (\n",
    "    customers_df.alias(\"customers\")\n",
    "    .join(orders_df.alias(\"orders\"), \"customerNumber\", \"left\")\n",
    "    .join(orderdetails_df.alias(\"order_details\"), \"orderNumber\", \"left\")\n",
    "    .join(payments_df.alias(\"payments\"), \"customerNumber\", \"left\")\n",
    "    .groupBy(\"customers.city\")\n",
    "    .agg(\n",
    "        countDistinct(\"customers.customerNumber\").alias(\"NUM_OF_CUSTOMERS\"),\n",
    "        sum(expr(\"order_details.priceEach * order_details.quantityOrdered\")).alias(\"TOTAL_SALES\")\n",
    "    )\n",
    "    .withColumnRenamed(\"customers.city\", \"CITY\")\n",
    "    .orderBy(\"TOTAL_SALES\", ascending=False)\n",
    ")\n",
    "\n",
    "df_maria.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78f4a813",
   "metadata": {},
   "source": [
    "### SNOWFLAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8712857e-11b7-4723-9e0d-1c309a34d7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 06:10:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------+\n",
      "|             CITY|NUM_OF_CUSTOMERS|TOTAL_SALES|\n",
      "+-----------------+----------------+-----------+\n",
      "|          Abilene|               1|      1.392|\n",
      "|           Elyria|               1|      1.824|\n",
      "|          Jupiter|               1|      2.064|\n",
      "|        Pensacola|               1|      2.214|\n",
      "|     Ormond Beach|               1|      2.808|\n",
      "|  San Luis Obispo|               1|       3.62|\n",
      "|       Springdale|               1|        4.3|\n",
      "|           Layton|               1|       4.96|\n",
      "|           Keller|               1|          6|\n",
      "|    Missouri City|               1|       6.37|\n",
      "|        Deer Park|               1|      6.924|\n",
      "|      Port Orange|               1|      7.824|\n",
      "|         Billings|               1|      8.288|\n",
      "|       Romeoville|               1|      8.952|\n",
      "|        Iowa City|               1|       9.99|\n",
      "|          Baytown|               1|     10.368|\n",
      "|        Rock Hill|               1|      11.85|\n",
      "|      Chapel Hill|               1|     14.016|\n",
      "|Arlington Heights|               1|     14.112|\n",
      "|    New Brunswick|               2|      14.77|\n",
      "+-----------------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Spark & SnowFlake Connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Snowflake Query\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/snowflake-jdbc-3.14.5.jar,/usr/local/spark/jars/spark-snowflake_2.13-2.14.0-spark_3.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Snowflake connection\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://tltmqjz-yk82271.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"YK82271\",\n",
    "  \"sfUser\" : \"HOWARDNGUYEN29\",\n",
    "  \"sfPassword\" : \"password\", #oi29\n",
    "  \"sfDatabase\" : \"GBCTRAINING3\",\n",
    "  \"sfSchema\" : \"PUBLIC\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\",\n",
    "}\n",
    "\n",
    "# Query 2 databases, one is .json and one is .csv, then union those\n",
    "query = \"\"\"\n",
    "    SELECT City, Num_of_customers, total_sales\n",
    "    FROM (\n",
    "        SELECT City, COUNT(DISTINCT Customers) AS Num_of_customers, SUM(quantity * price) AS total_sales\n",
    "        FROM (\n",
    "            SELECT \n",
    "                SAMPLE_SUPPLIES.STORELOCATION AS City,\n",
    "                SAMPLE_SUPPLIES.CUSTOMER AS Customers,\n",
    "                TO_NUMBER(GET(GET(ITEM.VALUE, 'quantity'), '$numberInt')) AS quantity,\n",
    "                TO_NUMBER(GET(GET(ITEM.VALUE, 'price'), '$numberDecimal')) AS price\n",
    "            FROM\n",
    "                SAMPLE_SUPPLIES,\n",
    "            LATERAL FLATTEN(INPUT => PARSE_JSON(SAMPLE_SUPPLIES.items)) AS item\n",
    "        ) AS subquery1\n",
    "        GROUP BY City\n",
    "\n",
    "        UNION\n",
    "\n",
    "        SELECT city, count(CUSTOMER_ID) AS Num_of_customers, SUM(sales) AS total_sales\n",
    "        FROM SUPERSTORE\n",
    "        GROUP BY city\n",
    "    ) AS combined_data\n",
    "    ORDER BY total_sales;\n",
    "\"\"\"\n",
    "\n",
    "# Load data from Snowflake using the defined query\n",
    "df_snowflake = spark.read.format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"query\", query) \\\n",
    "    .load()\n",
    "\n",
    "df_snowflake.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7865453f",
   "metadata": {},
   "source": [
    "### MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa400081-3f25-4014-badd-abfc271d74c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 06:11:00 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:00 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:00 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:03 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:03 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:03 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:04 WARN SnowflakeStrategy: Pushdown failed :null     (0 + 1) / 1]\n",
      "24/02/21 06:11:04 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:04 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:05 WARN SnowflakeStrategy: Pushdown failed :null                 \n",
      "24/02/21 06:11:05 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:05 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:06 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:06 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:06 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:07 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:07 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:07 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:08 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:08 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:08 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:09 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:09 WARN SnowflakeStrategy: Pushdown failed :null\n",
      "24/02/21 06:11:09 WARN SnowflakeStrategy: Pushdown failed :null\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------+\n",
      "|             CITY|NUM_OF_CUSTOMERS|TOTAL_SALES|\n",
      "+-----------------+----------------+-----------+\n",
      "|          Abilene|               1|      1.392|\n",
      "|           Elyria|               1|      1.824|\n",
      "|          Jupiter|               1|      2.064|\n",
      "|        Pensacola|               1|      2.214|\n",
      "|     Ormond Beach|               1|      2.808|\n",
      "|  San Luis Obispo|               1|       3.62|\n",
      "|       Springdale|               1|        4.3|\n",
      "|           Layton|               1|       4.96|\n",
      "|           Keller|               1|        6.0|\n",
      "|    Missouri City|               1|       6.37|\n",
      "|        Deer Park|               1|      6.924|\n",
      "|      Port Orange|               1|      7.824|\n",
      "|         Billings|               1|      8.288|\n",
      "|       Romeoville|               1|      8.952|\n",
      "|        Iowa City|               1|       9.99|\n",
      "|          Baytown|               1|     10.368|\n",
      "|        Rock Hill|               1|      11.85|\n",
      "|      Chapel Hill|               1|     14.016|\n",
      "|Arlington Heights|               1|     14.112|\n",
      "|    New Brunswick|               2|      14.77|\n",
      "+-----------------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merging\n",
    "merged_df = df_mongodb.union(df_hadoop).union(df_maria).union(df_snowflake)\n",
    "# merged_df = merged_df.withColumn(\"TOTAL_SALES\", format_number(\"TOTAL_SALES\", 2))\n",
    "\n",
    "# Filter out rows with null values in the total sales\n",
    "merged_df = merged_df.filter(merged_df.TOTAL_SALES.isNotNull())\n",
    "\n",
    "# Drop same records\n",
    "merged_df = merged_df.dropDuplicates([\"CITY\"])\n",
    "\n",
    "merged_df = merged_df.orderBy(\"TOTAL_SALES\")\n",
    "merged_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c9328c2",
   "metadata": {},
   "source": [
    "# Distribute merged DF into SSMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab3ab56b-cb50-4b2b-abe5-cae30f1f77c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyodbc\n",
      "  Downloading pyodbc-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Downloading pyodbc-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.7/334.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyodbc\n",
      "Successfully installed pyodbc-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy\n",
    "!pip install pyodbc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9e36e32",
   "metadata": {},
   "source":
    "## Should install ODBC+Driver+18+for+SQL+Server by https://learn.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16&tabs=alpine18-install%2Calpine17-install%2Cdebian8-install%2Credhat7-13-install%2Crhel7-offline (choos Ubuntu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dea434fa-80a6-4333-b4d3-5f1993a045ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# Collect data from Spark DF to the driver node\n",
    "merged_df_collected = merged_df.collect()\n",
    "\n",
    "# Convert collected data to Pandas DF\n",
    "pandas_df = pd.DataFrame(merged_df_collected, columns=merged_df.columns)\n",
    "\n",
    "# Define database connection string using ODBC\n",
    "connection_string = 'mssql+pyodbc://azureuser:password@howardserver.database.windows.net/myfirstdatabase?driver=ODBC+Driver+18+for+SQL+Server'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = sqlalchemy.create_engine(connection_string)\n",
    "\n",
    "# Connect to the database\n",
    "connection = engine.connect()\n",
    "\n",
    "# Define the table name\n",
    "table_name = 'SaleOnCity'\n",
    "\n",
    "# Create a table in the database based on the DF schema\n",
    "pandas_df.to_sql(table_name, connection, if_exists='replace', index=False)\n",
    "\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
